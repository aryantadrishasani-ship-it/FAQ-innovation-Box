# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A6VCUU5rS0QhreP_5Zv-PR7tRaX6_C2c
"""

import os, requests

os.environ["OPENROUTER_API_KEY"] = "sk-or-v1-sk-or-v1-xxx"

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_MODEL = "meta-llama/llama-3.1-8b-instruct:free"

url = "https://openrouter.ai/api/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {os.getenv('OPENROUTER_API_KEY')}",
    "HTTP-Referer": "https://colab.research.google.com/",
    "X-Title": "test"
}
data = {
    "model": "meta-llama/llama-3.1-8b-instruct:free",
    "messages": [
        {"role": "user", "content": "Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ú¯Ùˆ Ø³Ù„Ø§Ù…ØŒ Ø§ØªØµØ§Ù„ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø§Ø³Øª"}
    ],
    "max_tokens": 50
}

r = requests.post(url, headers=headers, json=data)
print(r.status_code)
print(r.json())

# Naive RAG Ú©Ù…â€ŒØ­Ø¬Ù… Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Google Colab
# Ú©Ù…ØªØ±ÛŒÙ† Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ù…Ú©Ù† + ÙØ§Ø±Ø³ÛŒ Ø¹Ø§Ù„ÛŒ
'''
Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ embedding: ÙÙ‚Ø· ~90 Ù…Ú¯Ø§Ø¨Ø§ÛŒØª
Ø¯ÛŒØªØ§Ø¨ÛŒØ³: ChromaDB â†’ Ø³Ø¨Ú©ØŒ Ø³Ø±ÛŒØ¹ØŒ Ø¨Ø¯ÙˆÙ† Ø¯Ø±Ø¯Ø³Ø±
LLM:
Ø¨Ø§ OpenRouter â†’ Ø¯Ø§Ù†Ù„ÙˆØ¯ ØµÙØ± + ÙØ§Ø±Ø³ÛŒ Ø¹Ø§Ù„ÛŒ
Ø¨Ø¯ÙˆÙ† OpenRouter â†’ ÙÙ‚Ø· ~1 GB Ø¯Ø§Ù†Ù„ÙˆØ¯ ÛŒÚ©â€ŒØ¨Ø§Ø± (Qwen2-0.5B)
'''
!pip install -q sentence-transformers chromadb pandas openpyxl requests

import pandas as pd
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from google.colab import files
import io
import requests
import json
from transformers import pipeline

# ============= ØªÙ†Ø¸ÛŒÙ…Ø§Øª =============
USE_OPENROUTER = True  # True â†’ Ø¨Ù‡ØªØ±ÛŒÙ† Ú©ÛŒÙÛŒØª Ùˆ ÙØ§Ø±Ø³ÛŒ Ø¹Ø§Ù„ÛŒ (Ø¯Ø§Ù†Ù„ÙˆØ¯ ØµÙØ±)
                       # False â†’ Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ø­Ù„ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ø³Ø¨Ú©

# ÙÙ‚Ø· Ø§Ú¯Ø± USE_OPENROUTER = True Ø¨Ø§Ø´Ù‡ØŒ Ú©Ù„ÛŒØ¯ Ù„Ø§Ø²Ù… Ø§Ø³Øª (Ø±Ø§ÛŒÚ¯Ø§Ù† Ø«Ø¨Øªâ€ŒÙ†Ø§Ù… Ú©Ù† Ø¯Ø± openrouter.ai)
OPENROUTER_API_KEY = "sk-or-v1-sk-or-v1-xxx"  # â† Ø§ÛŒÙ†Ø¬Ø§ Ú©Ù„ÛŒØ¯Øª Ø±Ùˆ Ø¨Ø°Ø§Ø± (ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¨Ø°Ø§Ø± Ùˆ USE_OPENROUTER=False Ú©Ù†)

OPENROUTER_MODEL = "meta-llama/llama-3.1-8b-instruct:free"  # Ø±Ø§ÛŒÚ¯Ø§Ù† + ÙØ§Ø±Ø³ÛŒ ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡

TOP_K = 5  # ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ Ù…Ø±ØªØ¨Ø· (Ûµ Ø¹Ø§Ù„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„)
# ===================================

# Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ FAQ
print("ğŸ“‚ Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ FAQ Ø®ÙˆØ¯ØªÙˆÙ† Ø±Ùˆ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ (CSV ÛŒØ§ Excel)")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
print(f"ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯: {file_name}")

# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„
if file_name.endswith('.xlsx'):
    df = pd.read_excel(io.BytesIO(uploaded[file_name]))
else:
    df = pd.read_csv(io.BytesIO(uploaded[file_name]))

# ØªÙ†Ø¸ÛŒÙ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ (Ø§Ú¯Ø± ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÙ†ØŒ Ø§ÛŒÙ†Ø¬Ø§ Ø§ØµÙ„Ø§Ø­ Ú©Ù†)
df.columns = ['category', 'question', 'answer']  # â† Ø§Ú¯Ø± Ø§Ø³Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ ÙØ±Ù‚ Ø¯Ø§Ø±Ù‡ØŒ ØªØºÛŒÛŒØ± Ø¨Ø¯Ù‡
df = df.astype(str).apply(lambda x: x.str.strip())

print(f"âœ… {len(df)} Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.\n")

# Ù…Ø±Ø­Ù„Ù‡ Û²: Ø³Ø§Ø®Øª Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
docs = df.apply(lambda row: f"Ø³ÙˆØ§Ù„: {row['question']}\nÙ¾Ø§Ø³Ø®: {row['answer']}\nØ¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ: {row['category']}", axis=1).tolist()

# Ù…Ø±Ø­Ù„Ù‡ Û³: Ù…Ø¯Ù„ Embedding Ú©Ù…â€ŒØ­Ø¬Ù… Ùˆ ÙØ§Ø±Ø³ÛŒâ€ŒØ¯ÙˆØ³Øª
print("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ embedding (ÙÙ‚Ø· ~90 Ù…Ú¯Ø§Ø¨Ø§ÛŒØª)...")
embedding_function = SentenceTransformerEmbeddingFunction(model_name="paraphrase-multilingual-MiniLM-L12-v2")

# Ù…Ø±Ø­Ù„Ù‡ Û´: Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ChromaDB (Ø³Ø¨Ú©â€ŒØªØ± Ø§Ø² FAISS)
print("Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ ÙˆÚ©ØªÙˆØ±ÛŒ (ChromaDB)...")
chroma_client = chromadb.Client()  # Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
collection = chroma_client.create_collection(
    name="faq_collection",
    embedding_function=embedding_function,
    get_or_create=True
)

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ø§Ø³Ù†Ø§Ø¯
collection.add(
    documents=docs,
    ids=[f"doc_{i}" for i in range(len(docs))]
)

print(f"âœ… Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ {len(docs)} Ø³Ù†Ø¯ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯.\n")

# Ù…Ø±Ø­Ù„Ù‡ Ûµ: ØªØ§Ø¨Ø¹ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
def generate_answer(query: str, context: str) -> str:
    if USE_OPENROUTER and OPENROUTER_API_KEY != "sk-or-v1-sk-or-v1-xxx":
        # OpenRouter â†’ Ø¨Ù‡ØªØ±ÛŒÙ† Ú©ÛŒÙÛŒØªØŒ ÙØ§Ø±Ø³ÛŒ Ø¹Ø§Ù„ÛŒØŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ ØµÙØ±
        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "HTTP-Referer": "https://colab.research.google.com/",
            "X-Title": "Naive RAG Ù¾Ø±ÙˆÚ˜Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ"
        }
        data = {
            "model": OPENROUTER_MODEL,
            "messages": [
                {"role": "system", "content": "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø¯Ù‚ÛŒÙ‚ Ù‡Ø³ØªÛŒØ¯. ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯. Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†ÛŒØ³ØªØŒ Ø¨Ú¯ÙˆÛŒÛŒØ¯ 'Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ø± FAQ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª'."},
                {"role": "user", "content": f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª:\n{context}\n\nØ³ÙˆØ§Ù„: {query}\n\nÙ¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ú©Ø§Ù…Ù„:"}
            ],
            "temperature": 0.4,
            "max_tokens": 600
        }
        try:
            response = requests.post(url, headers=headers, json=data, timeout=60)
            if response.status_code == 200:
                return response.json()['choices'][0]['message']['content'].strip()
            else:
                return f"Ø®Ø·Ø§ Ø¯Ø± OpenRouter: {response.status_code}\nÙ…ÛŒâ€ŒØªÙˆÙ†ÛŒØ¯ USE_OPENROUTER=False Ú©Ù†ÛŒØ¯ Ùˆ Ø§Ø² Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
        except:
            return "Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ OpenRouter. Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ ÙØ¹Ø§Ù„ Ø´Ø¯."

    else:
        # Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Ø³Ø¨Ú© (Qwen2 0.5B) â€” ÙØ§Ø±Ø³ÛŒ Ø®ÛŒÙ„ÛŒ Ø¨Ù‡ØªØ± Ø§Ø² TinyLlama
        print("(Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Qwen2-0.5B â€” ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø·ÙˆÙ„ Ù…ÛŒâ€ŒÚ©Ø´Ù‡)")
        generator = pipeline(
            "text-generation",
            model="Qwen/Qwen2-0.5B-Instruct",
            device=-1  # CPU
        )
        prompt = f"""You are a helpful assistant. Answer in Persian based only on the provided context.

Context:
{context}

Question: {query}

Answer:"""
        result = generator(prompt, max_new_tokens=400, temperature=0.6, do_sample=True)
        return result[0]['generated_text'].split("Answer:")[-1].strip()

# Ù…Ø±Ø­Ù„Ù‡ Û¶: Ø­Ù„Ù‚Ù‡ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®
print("ğŸ¤– Ø³ÛŒØ³ØªÙ… Naive RAG Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!\n")
print(f"Ø­Ø§Ù„Øª LLM: {'OpenRouter (Llama 3.1 8B Ø±Ø§ÛŒÚ¯Ø§Ù† â€” ÙØ§Ø±Ø³ÛŒ Ø¹Ø§Ù„ÛŒ)' if USE_OPENROUTER else 'Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Qwen2-0.5B (Ø³Ø¨Ú© Ùˆ Ø¢ÙÙ„Ø§ÛŒÙ†)'}\n")

while True:
    query = input("Ø³ÙˆØ§Ù„ Ø´Ù…Ø§: ").strip()

    if query.lower() in ["exit", "Ø®Ø±ÙˆØ¬", "quit", "bye"]:
        print("Ù…ÙˆÙÙ‚ Ø¨Ø§Ø´ÛŒ! ğŸ‘‹")
        break

    if not query:
        print("Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ÛŒ Ø¨Ù¾Ø±Ø³ÛŒØ¯.\n")
        continue

    print("\nØ¬Ø³ØªØ¬Ùˆ Ø¯Ø± FAQ...")
    results = collection.query(query_texts=[query], n_results=TOP_K)
    context_parts = results['documents'][0]
    context = "\n\n".join(context_parts)

    print("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®...")
    answer = generate_answer(query, context)

    print("\nğŸ“Œ Ù¾Ø§Ø³Ø®:")
    print(answer)
    print("\n" + "â€”" * 80 + "\n")

